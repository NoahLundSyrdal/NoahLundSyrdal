{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0iVpOFAytxD"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this notebook, we'll implement implement the forward pass of an SSM (State Space Model) using recursion and convolution based approaches. We'll also compare the two approaches in terms of speed and memory usage.\n",
        "\n",
        "You will need GPU for this notebook which can be enabled via changing the runtime.\n",
        "\n",
        "You can copy your solutions from the `q_coding_ssm_forward_cpu` notebook for the first part of the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIngxUwVytxE"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-12T21:21:09.969698Z",
          "iopub.status.busy": "2025-03-12T21:21:09.969368Z",
          "iopub.status.idle": "2025-03-12T21:21:14.057692Z",
          "shell.execute_reply": "2025-03-12T21:21:14.056717Z",
          "shell.execute_reply.started": "2025-03-12T21:21:09.969663Z"
        },
        "id": "-NnY8J6HytxE",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.set_default_device('cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRxe5357ytxF"
      },
      "source": [
        "# SSM Update Rule\n",
        "\n",
        "We consider an RNN described by the update:\n",
        "\n",
        "$$\n",
        "h_{t+1} = W\\,h_t + U\\,x_t + b\n",
        "$$\n",
        "\n",
        "for $t = 0, 1, \\ldots, T - 1$. The variables are:\n",
        "\n",
        "- $h_t \\in R^H$, the hidden state at time $t$.\n",
        "- $x_t \\in R^{N \\times D}$, the input at time $t$.\n",
        "- $W \\in R^{H \\times H}$, the recurrent weight matrix.\n",
        "- $U \\in R^{H \\times D}$, the input projection matrix.\n",
        "- $b \\in R^H$, the bias vector.\n",
        "\n",
        "$N$ is the batch size, $D$ is the input dimension, and $H$ is the hidden state dimension. We assume $h_0 = 0$, the all-zero vector of dimension $H$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUHIQVVUytxF"
      },
      "source": [
        "Below you will implement the forward pass for the SSM using recursion based approach. The `unrolled_ssm_forward` function will take weights $W$, $U$, $b$ and input $x$ and return the hidden states $h$ across different time steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-12T21:21:16.954857Z",
          "iopub.status.busy": "2025-03-12T21:21:16.954402Z",
          "iopub.status.idle": "2025-03-12T21:21:16.962704Z",
          "shell.execute_reply": "2025-03-12T21:21:16.961525Z",
          "shell.execute_reply.started": "2025-03-12T21:21:16.954797Z"
        },
        "id": "R8phBnwkytxF",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def unrolled_ssm_forward(W, U, b, x):\n",
        "    \"\"\"\n",
        "    Unroll the linear RNN in time:\n",
        "        h_{t+1} = W h_t + U x_t + b\n",
        "    with initial h_0 = 0.\n",
        "\n",
        "    Args:\n",
        "      W: (H, H) weight matrix\n",
        "      U: (H, D) input projection\n",
        "      b: (H,)   bias\n",
        "      x: (N, T, D) input sequence over T steps\n",
        "    Returns:\n",
        "      h_all: (N, T, H) hidden states for t=1..T\n",
        "             (h_all[t] corresponds to h_{t+1} in the usual notation).\n",
        "    \"\"\"\n",
        "    N, T, _ = x.shape\n",
        "    H = b.shape[0]\n",
        "    h = torch.zeros((N, H), dtype=x.dtype, device=x.device)\n",
        "    h_all = torch.zeros((N, T, H), dtype=x.dtype, device=x.device)\n",
        "    for t in range(T):\n",
        "        h = h @ W.T + x[:, t, :] @ U.T + b\n",
        "        h_all[:, t, :] = h\n",
        "    return h_all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-XNCy-jytxF"
      },
      "source": [
        "# Convolution Based Implementation\n",
        "\n",
        "In the previous problem, you showed that the forward pass of an SSM can be implmemented using a convolution operation. In this problem, you will implement the forward pass of an SSM using a convolution based approach. You can assume that T is a power of 2.\n",
        "\n",
        "\n",
        "You will implement two functions\n",
        "- `make_conv_kernel(W, T)`: This function will take the recurrent weight matrix $W$ and the number of time steps $T$ and return the convolution kernel $K$. Given that T is a power of 2, you can implement this using a divide and conquer based approach.\n",
        "- `conv_ssm_forward(W, U, b, x)`: This function will take weights $W$, $U$, $b$ and input $x$ and return the hidden states $h$ across different time steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-12T21:21:18.632154Z",
          "iopub.status.busy": "2025-03-12T21:21:18.631819Z",
          "iopub.status.idle": "2025-03-12T21:21:18.638878Z",
          "shell.execute_reply": "2025-03-12T21:21:18.637950Z",
          "shell.execute_reply.started": "2025-03-12T21:21:18.632130Z"
        },
        "id": "pBRUBYUe2e2y",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def make_conv_kernel(W, T):\n",
        "    \"\"\"\n",
        "    Build a 3D kernel tensor K of shape (H, H, T) we will use when implementing\n",
        "    the ssm forward pass using conv1d.\n",
        "\n",
        "    Args:\n",
        "      W: (H, H) weight matrix\n",
        "      T: scalar\n",
        "\n",
        "    Returns:\n",
        "      kernel_for_conv: (H, H, T) tensor\n",
        "    \"\"\"\n",
        "    H = W.shape[0]\n",
        "    K = torch.zeros((H, H, T), dtype=W.dtype, device=W.device)\n",
        "    P = torch.eye(H, dtype=W.dtype, device=W.device)  # W^0\n",
        "    powers = [P]\n",
        "    for _ in range(1, T):\n",
        "        P = W @ P\n",
        "        powers.append(P)\n",
        "    for k in range(T):\n",
        "        K[:, :, k] = powers[T - 1 - k]\n",
        "    return K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-12T21:21:20.408043Z",
          "iopub.status.busy": "2025-03-12T21:21:20.407722Z",
          "iopub.status.idle": "2025-03-12T21:21:20.412975Z",
          "shell.execute_reply": "2025-03-12T21:21:20.412100Z",
          "shell.execute_reply.started": "2025-03-12T21:21:20.408016Z"
        },
        "id": "9bXNEWxfytxF",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def conv_ssm_forward(W, U, b, x):\n",
        "    \"\"\"\n",
        "    Convolution-based forward pass for a batch of sequences.\n",
        "\n",
        "    RNN update:  h_{t+1} = W h_t + U x_t + b\n",
        "\n",
        "    Args:\n",
        "      W: (H, H) weight matrix\n",
        "      U: (H, D) input projection\n",
        "      b: (H,)   bias\n",
        "      x: (N, T, D) input (batch=N, time steps=T, input dim=D)\n",
        "\n",
        "    Returns:\n",
        "      h_all: (N, T, H) hidden states\n",
        "    \"\"\"\n",
        "    N, T, D = x.shape\n",
        "    H = W.shape[0]\n",
        "    s = x @ U.T + b  # (N, T, H), broadcasts b\n",
        "    K = make_conv_kernel(W, T)  # (H, H, T) with K[:,:,k] = W^{T-1-k}\n",
        "\n",
        "    # Emulate conv1d with padding=T-1 and take first T outputs:\n",
        "    # y[n] = sum_{i=0}^n W^{(n-i)} s[i]\n",
        "    h_all = torch.zeros((N, T, H), dtype=s.dtype, device=s.device)\n",
        "    # Precompute W^k from kernel (reverse mapping)\n",
        "    # W^{lag} = K[:, :, T-1 - lag]\n",
        "    for n in range(T):\n",
        "        acc = torch.zeros((N, H), dtype=s.dtype, device=s.device)\n",
        "        for i in range(n + 1):\n",
        "            lag = n - i\n",
        "            Wlag = K[:, :, T - 1 - lag]  # (H, H)\n",
        "            acc += s[:, i, :] @ Wlag.T\n",
        "        h_all[:, n, :] = acc\n",
        "    return h_all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmcD4eJPytxG"
      },
      "source": [
        "# Sanity Check\n",
        "We can compare the outputs of the two implementations to check if they are consistent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-03-12T21:21:23.176996Z",
          "iopub.status.busy": "2025-03-12T21:21:23.176662Z",
          "iopub.status.idle": "2025-03-12T21:21:24.480211Z",
          "shell.execute_reply": "2025-03-12T21:21:24.479403Z",
          "shell.execute_reply.started": "2025-03-12T21:21:23.176970Z"
        },
        "id": "cbBabsamytxG",
        "outputId": "5d380fdb-d626-45f4-a7ee-cd99d0eae62a",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unrolled h(t):\n",
            "tensor([[[ 0.2619, -0.3655,  0.0208,  0.2176],\n",
            "         [ 0.3612, -0.1343,  0.1213,  0.1395],\n",
            "         [ 0.1538, -0.0023, -0.1271, -0.0176],\n",
            "         [ 0.3927, -0.1537,  0.1103,  0.0732],\n",
            "         [ 0.0711, -0.0081, -0.2333, -0.0349],\n",
            "         [ 0.3832,  0.1360, -0.0033, -0.1085],\n",
            "         [ 0.3630, -0.0947,  0.0646,  0.0288],\n",
            "         [ 0.1400,  0.0112, -0.1930, -0.0218]],\n",
            "\n",
            "        [[-0.0384,  0.0034, -0.3276, -0.1926],\n",
            "         [ 0.3462,  0.0823, -0.0832, -0.0996],\n",
            "         [ 0.1896,  0.2200, -0.2117, -0.2180],\n",
            "         [ 0.0559,  0.0817, -0.3868, -0.1649],\n",
            "         [ 0.4200, -0.4191,  0.1368,  0.2055],\n",
            "         [ 0.2792, -0.0009,  0.0220,  0.0395],\n",
            "         [ 0.1535, -0.3252, -0.0715,  0.1442],\n",
            "         [ 0.2363, -0.3806,  0.0232,  0.2383]]], device='cuda:0')\n",
            "\n",
            "Conv-based h(t):\n",
            "tensor([[[ 0.2619, -0.3655,  0.0208,  0.2176],\n",
            "         [ 0.3612, -0.1343,  0.1213,  0.1395],\n",
            "         [ 0.1538, -0.0023, -0.1271, -0.0176],\n",
            "         [ 0.3927, -0.1537,  0.1103,  0.0732],\n",
            "         [ 0.0711, -0.0081, -0.2333, -0.0349],\n",
            "         [ 0.3832,  0.1360, -0.0033, -0.1085],\n",
            "         [ 0.3630, -0.0947,  0.0646,  0.0288],\n",
            "         [ 0.1400,  0.0112, -0.1930, -0.0218]],\n",
            "\n",
            "        [[-0.0384,  0.0034, -0.3276, -0.1926],\n",
            "         [ 0.3462,  0.0823, -0.0832, -0.0996],\n",
            "         [ 0.1896,  0.2200, -0.2117, -0.2180],\n",
            "         [ 0.0559,  0.0817, -0.3868, -0.1649],\n",
            "         [ 0.4200, -0.4191,  0.1368,  0.2055],\n",
            "         [ 0.2792, -0.0009,  0.0220,  0.0395],\n",
            "         [ 0.1535, -0.3252, -0.0715,  0.1442],\n",
            "         [ 0.2363, -0.3806,  0.0232,  0.2383]]], device='cuda:0')\n",
            "\n",
            "Max absolute difference: 2.9802322387695312e-08\n"
          ]
        }
      ],
      "source": [
        "def sanity_check():\n",
        "    T = 8   # number of time steps\n",
        "    H = 4   # hidden dimension\n",
        "    D = 3   # input dimension\n",
        "    N = 2\n",
        "\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    W = torch.randn(H, H) * 0.1\n",
        "    U = torch.randn(H, D) * 0.1\n",
        "    b = torch.randn(H) * 0.1\n",
        "\n",
        "    x = torch.randn(N, T, D)\n",
        "\n",
        "    h_unrolled = unrolled_ssm_forward(W, U, b, x)\n",
        "    h_conv = conv_ssm_forward(W, U, b, x)\n",
        "\n",
        "    diff = (h_unrolled - h_conv).abs().max()\n",
        "    print(\"Unrolled h(t):\")\n",
        "    print(h_unrolled)\n",
        "    print(\"\\nConv-based h(t):\")\n",
        "    print(h_conv)\n",
        "    print(\"\\nMax absolute difference:\", diff.item())\n",
        "\n",
        "sanity_check()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axpXeGcNytxG"
      },
      "source": [
        "# Implementation Complexity\n",
        "\n",
        "We can compare the two implementations in terms of efficiency. Particularly, we will compare the time taken by the two implementations to compute the hidden states for a given input and weights with varying number of time steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-12T21:26:02.117139Z",
          "iopub.status.busy": "2025-03-12T21:26:02.116802Z",
          "iopub.status.idle": "2025-03-12T21:26:47.377750Z",
          "shell.execute_reply": "2025-03-12T21:26:47.377035Z",
          "shell.execute_reply.started": "2025-03-12T21:26:02.117113Z"
        },
        "id": "Vgh6sC3p5UqJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact\n",
        "\n",
        "def measure_runtime(method_fn, W, U, b, x, warmup=1, repeats=10):\n",
        "    # Warm-up runs (ignored in timing):\n",
        "    for _ in range(warmup):\n",
        "        method_fn(W, U, b, x)\n",
        "\n",
        "    # Timed runs:\n",
        "    start = time.time()\n",
        "    for _ in range(repeats):\n",
        "        method_fn(W, U, b, x)\n",
        "    end = time.time()\n",
        "\n",
        "    avg_time = (end - start) / repeats\n",
        "    return avg_time\n",
        "\n",
        "\n",
        "def run():\n",
        "    T_values_cache = {}\n",
        "    times_unrolled_vs_T_cache = {}\n",
        "    times_conv_vs_T_cache = {}\n",
        "\n",
        "    for H in [4, 8, 16, 32, 64, 128, 256, 512]:\n",
        "      # We'll keep D, N fixed\n",
        "      D = 32\n",
        "      N = 32\n",
        "\n",
        "      T_values = [8, 32, 128, 256, 512]\n",
        "\n",
        "      # Build random U, b\n",
        "      U = torch.randn(H, D)*0.1\n",
        "      b = torch.randn(H)*0.1\n",
        "\n",
        "      times_unrolled_vs_T = []\n",
        "      times_conv_vs_T = []\n",
        "\n",
        "      for T in T_values:\n",
        "\n",
        "          diag_vals = torch.randn(H)*0.05\n",
        "          W = torch.randn(H, H)*0.05\n",
        "          x = torch.randn(N, T, D)\n",
        "\n",
        "          t_unrolled = measure_runtime(unrolled_ssm_forward, W, U, b, x)\n",
        "\n",
        "          t_conv = measure_runtime(conv_ssm_forward, W, U, b, x)\n",
        "\n",
        "          times_unrolled_vs_T.append(t_unrolled)\n",
        "          times_conv_vs_T.append(t_conv)\n",
        "\n",
        "      T_values_cache[H] = T_values\n",
        "      times_unrolled_vs_T_cache[H] = times_unrolled_vs_T\n",
        "      times_conv_vs_T_cache[H] = times_conv_vs_T\n",
        "    return T_values_cache, times_unrolled_vs_T_cache, times_conv_vs_T_cache\n",
        "\n",
        "T_values_cache, times_unrolled_vs_T_cache, times_conv_vs_T_cache = run()\n",
        "\n",
        "@interact(H=widgets.FloatLogSlider(min=2, max=9, base=2, value=4, step=1))\n",
        "def interactive_benchmark(H):\n",
        "    \"\"\"\n",
        "    Compare unrolled vs. diagonal-convolution RNN forward for various T,\n",
        "    at a chosen hidden dimension H from the slider.\n",
        "    \"\"\"\n",
        "    H = int(H)\n",
        "    T_values = T_values_cache[H]\n",
        "    T_unrolled = times_unrolled_vs_T_cache[H]\n",
        "    T_conv = times_conv_vs_T_cache[H]\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(T_values, T_unrolled, label=\"Unrolled\", marker='o')\n",
        "    plt.plot(T_values, T_conv, label=\"Conv\", marker='s')\n",
        "    plt.title(f\"Runtime vs T, H={H}\")\n",
        "    plt.xlabel(\"Time Steps (T)\")\n",
        "    plt.ylabel(\"Runtime (sec)\")\n",
        "    plt.yscale('log')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFu8Leoo7URC"
      },
      "source": [
        "### Question 6\n",
        "\n",
        "What do you observe about the runtime of the two implementations as $T$ and $H$ increase? Do you observe the same trend as the CPU notebook? Explain your reasoning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyb2V2wR8QHw"
      },
      "source": [
        "# Introducing structure: Diagonal Weight Matrics\n",
        "\n",
        "We can optimize the convolution implemntation via adding a constraint on the $W$ matrix ensuring it is diagonal. We can make the convolution implementation more efficient by leveraging depthwise convolutions for implementing the forward operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZJ2eEhY9Nps"
      },
      "outputs": [],
      "source": [
        "def make_diag_depthwise_kernel(W, T):\n",
        "    \"\"\"\n",
        "    Construct a depthwise 1D conv kernel for W with shape (H,H). W is a diagonal matrix.\n",
        "\n",
        "    Args:\n",
        "      W: (H, H) diagonal matrix\n",
        "      T: (int) number of time steps\n",
        "\n",
        "    Return:\n",
        "      kernel of shape (H, 1, T), which can be used in depthwise convolution\n",
        "    \"\"\"\n",
        "    ##############################################################################\n",
        "    #                         TODO: Implement the kernel here                    #\n",
        "    ##############################################################################\n",
        "    H = W.shape[0]\n",
        "    # Extract the diagonal elements\n",
        "    diagW = torch.diag(W) # (H,)\n",
        "\n",
        "    # Compute powers of the diagonal elements\n",
        "    # diagW^k = (diagW[0]^k, diagW[1]^k, ..., diagW[H-1]^k)\n",
        "    powers = torch.stack([diagW**k for k in range(T)], dim=1) # (H, T)\n",
        "\n",
        "    # The kernel K has K[:, :, k] = W^{T-1-k}. For a diagonal W,\n",
        "    # W^{T-1-k} is also diagonal with diag(W^{T-1-k})_i = diagW[i]**(T-1-k).\n",
        "    # So the kernel for depthwise convolution will have\n",
        "    # kernel[i, 0, k] = diagW[i]**(T-1-k)\n",
        "    kernel = powers.flip(dims=[1]).unsqueeze(1) # (H, 1, T)\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    return kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-12T21:26:47.379231Z",
          "iopub.status.busy": "2025-03-12T21:26:47.378946Z",
          "iopub.status.idle": "2025-03-12T21:26:47.388976Z",
          "shell.execute_reply": "2025-03-12T21:26:47.387914Z",
          "shell.execute_reply.started": "2025-03-12T21:26:47.379209Z"
        },
        "id": "wAd1StPf49ce",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def diag_conv_ssm_forward(W, U, b, x):\n",
        "    \"\"\"\n",
        "    Convolution-based forward pass for an RNN with W\n",
        "\n",
        "    RNN update:  h_{t+1} = W h_t + U x_t + b\n",
        "    but W is diagonal, so h_{t+1}(i) = diagW[i]*h_t(i) + [U x_t + b](i).\n",
        "\n",
        "    Args:\n",
        "      W: (H, H) [diagonal in practice]\n",
        "      U: (H, D)\n",
        "      b: (H,)\n",
        "      x: (N, T, D) => N = batch, T = time steps, D = input dim\n",
        "\n",
        "    Returns:\n",
        "      h_all: (N, T, H)\n",
        "    \"\"\"\n",
        "    N, T, D = x.shape\n",
        "    H = W.shape[0]\n",
        "\n",
        "    s = x @ U.T + b\n",
        "\n",
        "    s = s.permute(0, 2, 1)  # (N,H,T)\n",
        "\n",
        "    # Build kernel (H,1,T) for depthwise conv\n",
        "    kernel = make_diag_depthwise_kernel(W, T)\n",
        "    ##############################################################################\n",
        "    #                         TODO: Implement the convolution here               #\n",
        "    #                         Hint: Use `groups` argument                        #\n",
        "    ##############################################################################\n",
        "    h_all = F.conv1d(s, kernel, groups=H, padding=T-1)[:, :, :T]\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    return h_all.permute(0, 2, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dShADk7XA_eP"
      },
      "source": [
        "# Optimizing the recurrent implementation\n",
        "\n",
        "Similarly, we can also optimize the recurrent implementation by using the diagonal nature of the $W$ matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-12T21:26:47.391113Z",
          "iopub.status.busy": "2025-03-12T21:26:47.390752Z",
          "iopub.status.idle": "2025-03-12T21:26:47.407218Z",
          "shell.execute_reply": "2025-03-12T21:26:47.406254Z",
          "shell.execute_reply.started": "2025-03-12T21:26:47.391080Z"
        },
        "id": "iafrjZQ69sbb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def diag_unrolled_ssm_forward(W, U, b, x):\n",
        "    \"\"\"\n",
        "    Forward pass for:\n",
        "       h_{t+1} = W h_t + U x_t + b\n",
        "    but W is diagonal, i.e. W = diag(diagW).\n",
        "\n",
        "    Args:\n",
        "      W: (H, H) diagonal matrix\n",
        "      U: (H, D)\n",
        "      b: (H,)\n",
        "      x: (N, T, D)  => batch=N, time steps=T, input dim=D\n",
        "\n",
        "    Returns:\n",
        "      h_all: (N, T, H)\n",
        "    \"\"\"\n",
        "    ##############################################################################\n",
        "    #             TODO: Implement the optimized recurrent pass here              #\n",
        "    ##############################################################################\n",
        "    N, T, _ = x.shape\n",
        "    H = W.shape[0]\n",
        "    # Extract diagonal elements of W\n",
        "    diagW = torch.diag(W) # (H,)\n",
        "    h = torch.zeros((N, H), dtype=x.dtype, device=x.device)\n",
        "    h_all = torch.zeros((N, T, H), dtype=x.dtype, device=x.device)\n",
        "    for t in range(T):\n",
        "        # Optimized update using element-wise multiplication with diagonal W\n",
        "        h = h * diagW + x[:, t, :] @ U.T + b\n",
        "        h_all[:, t, :] = h\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    return h_all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boSFN0jYCA7w"
      },
      "source": [
        "# Sanity Check\n",
        "\n",
        "We can compare the outputs of the two implementations to check if they are consistent similar to above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-12T21:26:47.408676Z",
          "iopub.status.busy": "2025-03-12T21:26:47.408326Z",
          "iopub.status.idle": "2025-03-12T21:27:27.011936Z",
          "shell.execute_reply": "2025-03-12T21:27:27.011102Z",
          "shell.execute_reply.started": "2025-03-12T21:26:47.408638Z"
        },
        "id": "6035Z9X_-E9u",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def diag_sanity_check():\n",
        "    T = 8   # number of time steps\n",
        "    H = 4   # hidden dimension\n",
        "    D = 3   # input dimension\n",
        "    N = 2\n",
        "\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    W = torch.eye(H) * 0.1 #### WE USE A DIAGONAL MATRIX HERE\n",
        "\n",
        "    U = torch.randn(H, D) * 0.1\n",
        "    b = torch.randn(H) * 0.1\n",
        "\n",
        "    x = torch.randn(N, T, D)\n",
        "\n",
        "    h_unrolled = diag_unrolled_ssm_forward(W, U, b, x)\n",
        "\n",
        "    h_conv = diag_conv_ssm_forward(W, U, b, x)\n",
        "\n",
        "    diff = (h_unrolled - h_conv).abs().max()\n",
        "    print(\"Unrolled h(t):\")\n",
        "    print(h_unrolled)\n",
        "    print(\"\\nConv-based h(t):\")\n",
        "    print(h_conv)\n",
        "    print(\"\\nMax absolute difference:\", diff.item())\n",
        "\n",
        "diag_sanity_check()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU_buYdy8vhw"
      },
      "source": [
        "# Measure Runtime with Optimization\n",
        "\n",
        "Similarly, we will measure performance optimization with the diagonalized implementations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-12T21:27:27.013154Z",
          "iopub.status.busy": "2025-03-12T21:27:27.012816Z",
          "iopub.status.idle": "2025-03-12T21:27:41.463760Z",
          "shell.execute_reply": "2025-03-12T21:27:41.462903Z",
          "shell.execute_reply.started": "2025-03-12T21:27:27.013121Z"
        },
        "id": "APSy41yT5vEx",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def diag_run():\n",
        "    T_values_cache = {}\n",
        "    times_unrolled_vs_T_cache = {}\n",
        "    times_conv_vs_T_cache = {}\n",
        "\n",
        "    for H in [4, 8, 16, 32, 64, 128, 256, 512]:\n",
        "      # We'll keep D, N fixed\n",
        "      D = 32\n",
        "      N = 512\n",
        "\n",
        "      T_values = [8, 32, 128, 256, 512]\n",
        "\n",
        "      # Build random U, b\n",
        "      U = torch.randn(H, D)*0.1\n",
        "      b = torch.randn(H)*0.1\n",
        "\n",
        "      times_unrolled_vs_T = []\n",
        "      times_conv_vs_T = []\n",
        "\n",
        "      for T in T_values:\n",
        "          diag_vals = torch.randn(H)*0.05\n",
        "\n",
        "          W = torch.eye(H, H)*0.05 #### WE USE A DIAGONAL MATRIX HERE\n",
        "\n",
        "          x = torch.randn(N, T, D)\n",
        "\n",
        "          t_unrolled = measure_runtime(diag_unrolled_ssm_forward, W, U, b, x)\n",
        "\n",
        "          t_conv = measure_runtime(diag_conv_ssm_forward, W, U, b, x)\n",
        "\n",
        "          times_unrolled_vs_T.append(t_unrolled)\n",
        "          times_conv_vs_T.append(t_conv)\n",
        "\n",
        "      T_values_cache[H] = T_values\n",
        "      times_unrolled_vs_T_cache[H] = times_unrolled_vs_T\n",
        "      times_conv_vs_T_cache[H] = times_conv_vs_T\n",
        "    return T_values_cache, times_unrolled_vs_T_cache, times_conv_vs_T_cache\n",
        "\n",
        "diag_T_values_cache, diag_times_unrolled_vs_T_cache, diag_times_conv_vs_T_cache = diag_run()\n",
        "\n",
        "@interact(H=widgets.FloatLogSlider(min=2, max=9, base=2, value=4, step=1))\n",
        "def interactive_benchmark(H):\n",
        "    \"\"\"\n",
        "    Compare unrolled vs. diagonal-convolution RNN forward for various T,\n",
        "    at a chosen hidden dimension H from the slider.\n",
        "    \"\"\"\n",
        "    H = int(H)\n",
        "    T_values = diag_T_values_cache[H]\n",
        "    T_unrolled = diag_times_unrolled_vs_T_cache[H]\n",
        "    T_conv = diag_times_conv_vs_T_cache[H]\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(T_values, T_unrolled, label=\"Unrolled\", marker='o')\n",
        "    plt.plot(T_values, T_conv, label=\"Diag-Conv\", marker='s')\n",
        "    plt.title(f\"Runtime vs T, H={H} diagonal weights\")\n",
        "    plt.xlabel(\"Time Steps (T)\")\n",
        "    plt.ylabel(\"Runtime (sec)\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZCmvFSfCs-V"
      },
      "source": [
        "### Question 7\n",
        "\n",
        "What do you observe here? How do your findings different from the unstructured matrix case? Explain your reasoning."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "npy",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}