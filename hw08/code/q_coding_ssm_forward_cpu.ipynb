{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0iVpOFAytxD"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this notebook, we'll implement implement the forward pass of an SSM (State Space Model) using recursion and convolution based approaches. We'll also compare the two approaches in terms of speed and memory usage.\n",
        "\n",
        "You will use CPU for this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIngxUwVytxE"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-12T21:21:09.969698Z",
          "iopub.status.busy": "2025-03-12T21:21:09.969368Z",
          "iopub.status.idle": "2025-03-12T21:21:14.057692Z",
          "shell.execute_reply": "2025-03-12T21:21:14.056717Z",
          "shell.execute_reply.started": "2025-03-12T21:21:09.969663Z"
        },
        "id": "-NnY8J6HytxE",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRxe5357ytxF"
      },
      "source": [
        "# SSM Update Rule\n",
        "\n",
        "We consider an Linear RNN described by the update:\n",
        "\n",
        "$$\n",
        "h_{t+1} = W\\,h_t + U\\,x_t + b\n",
        "$$\n",
        "\n",
        "for $t = 0, 1, \\ldots, T - 1$. The variables are:\n",
        "\n",
        "- $h_t \\in R^H$, the hidden state at time $t$.\n",
        "- $x_t \\in R^{N \\times D}$, the input at time $t$.\n",
        "- $W \\in R^{H \\times H}$, the recurrent weight matrix.\n",
        "- $U \\in R^{H \\times D}$, the input projection matrix.\n",
        "- $b \\in R^H$, the bias vector.\n",
        "\n",
        "$N$ is the batch size, $D$ is the input dimension, and $H$ is the hidden state dimension. We assume $h_0 = 0$, the all-zero vector of dimension $H$."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aW37rOxT9Pp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "szeErtLI9PnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUHIQVVUytxF"
      },
      "source": [
        "Below you will implement the forward pass for the SSM using recursion based approach. The `unrolled_ssm_forward` function will take weights $W$, $U$, $b$ and input $x$ and return the hidden states $h$ across different time steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-12T21:21:16.954857Z",
          "iopub.status.busy": "2025-03-12T21:21:16.954402Z",
          "iopub.status.idle": "2025-03-12T21:21:16.962704Z",
          "shell.execute_reply": "2025-03-12T21:21:16.961525Z",
          "shell.execute_reply.started": "2025-03-12T21:21:16.954797Z"
        },
        "id": "R8phBnwkytxF",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def unrolled_ssm_forward(W, U, b, x):\n",
        "    \"\"\"\n",
        "    Unroll the linear RNN in time:\n",
        "        h_{t+1} = W h_t + U x_t + b\n",
        "    with initial h_0 = 0.\n",
        "\n",
        "    Args:\n",
        "      W: (H, H) weight matrix\n",
        "      U: (H, D) input projection\n",
        "      b: (H,)   bias\n",
        "      x: (N, T, D) input sequence over T steps\n",
        "    Returns:\n",
        "      h_all: (N, T, H) hidden states for t=1..T\n",
        "             (h_all[t] corresponds to h_{t+1} in the usual notation).\n",
        "    \"\"\"\n",
        "    ##############################################################################\n",
        "    #                   TODO: Implement the recurrent pass here                  #\n",
        "    ##############################################################################\n",
        "    N, T, D = x.shape\n",
        "    H = W.shape[0]\n",
        "\n",
        "    h = torch.zeros(N, H, device=x.device)  # Initial state h_0 = 0\n",
        "    h_all = torch.zeros(N, T, H, device=x.device)\n",
        "\n",
        "    for t in range(T):\n",
        "        h = h @ W.T + (x[:, t, :] @ U.T) + b\n",
        "        h_all[:, t, :] = h\n",
        "\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    return h_all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-XNCy-jytxF"
      },
      "source": [
        "# Convolution Based Implementation\n",
        "\n",
        "In the previous problem, you showed that the forward pass of an SSM can be implmemented using a convolution operation. In this problem, you will implement the forward pass of an SSM using a convolution based approach. You can assume that T is a power of 2.\n",
        "\n",
        "\n",
        "You will implement two functions\n",
        "- `make_conv_kernel(W, T)`: This function will take the recurrent weight matrix $W$ and the number of time steps $T$ and return the convolution kernel $K$. Given that T is a power of 2, you can implement this using a divide and conquer based approach.\n",
        "- `conv_ssm_forward(W, U, b, x)`: This function will take weights $W$, $U$, $b$ and input $x$ and return the hidden states $h$ across different time steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-12T21:21:18.632154Z",
          "iopub.status.busy": "2025-03-12T21:21:18.631819Z",
          "iopub.status.idle": "2025-03-12T21:21:18.638878Z",
          "shell.execute_reply": "2025-03-12T21:21:18.637950Z",
          "shell.execute_reply.started": "2025-03-12T21:21:18.632130Z"
        },
        "id": "pBRUBYUe2e2y",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def make_conv_kernel(W, T):\n",
        "    \"\"\"\n",
        "    Build a 3D kernel tensor K of shape (H, H, T) we will use when implementing\n",
        "    the ssm forward pass using conv1d.\n",
        "\n",
        "    Args:\n",
        "      W: (H, H) weight matrix\n",
        "      T: scalar\n",
        "\n",
        "    Returns:\n",
        "      kernel_for_conv: (H, H, T) tensor\n",
        "    \"\"\"\n",
        "    ##############################################################################\n",
        "    #                         TODO: Implement the kernel here                    #\n",
        "    ##############################################################################\n",
        "    H = W.shape[0]\n",
        "    kernel_for_conv = torch.zeros(H, H, T, device=W.device)\n",
        "\n",
        "    if T == 1:\n",
        "        kernel_for_conv[:, :, 0] = torch.eye(H, device=W.device)\n",
        "    else:\n",
        "        K_half = make_conv_kernel(W, T // 2)\n",
        "        kernel_for_conv[:, :, :T // 2] = W @ K_half\n",
        "        kernel_for_conv[:, :, T // 2:] = K_half\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    return kernel_for_conv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-12T21:21:20.408043Z",
          "iopub.status.busy": "2025-03-12T21:21:20.407722Z",
          "iopub.status.idle": "2025-03-12T21:21:20.412975Z",
          "shell.execute_reply": "2025-03-12T21:21:20.412100Z",
          "shell.execute_reply.started": "2025-03-12T21:21:20.408016Z"
        },
        "id": "9bXNEWxfytxF",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def conv_ssm_forward(W, U, b, x):\n",
        "    \"\"\"\n",
        "    Convolution-based forward pass for a batch of sequences.\n",
        "\n",
        "    RNN update:  h_{t+1} = W h_t + U x_t + b\n",
        "\n",
        "    Args:\n",
        "      W: (H, H) weight matrix\n",
        "      U: (H, D) input projection\n",
        "      b: (H,)   bias\n",
        "      x: (N, T, D) input (batch=N, time steps=T, input dim=D)\n",
        "\n",
        "    Returns:\n",
        "      h_all: (N, T, H) hidden states\n",
        "    \"\"\"\n",
        "    N, T, D = x.shape\n",
        "    H = W.shape[0]\n",
        "\n",
        "    s = x @ U.T + b\n",
        "\n",
        "    s = s.permute(0, 2, 1)\n",
        "\n",
        "    # Build the kernel with shape (H, H, T).\n",
        "    kernel = make_conv_kernel(W, T)\n",
        "    ##############################################################################\n",
        "    #                         TODO: Implement the convolution here               #\n",
        "    ##############################################################################\n",
        "    # Add padding to the input to handle the full convolution\n",
        "    s_padded = F.pad(s, (T - 1, 0))\n",
        "\n",
        "    # Reshape kernel for conv1d: (out_channels, in_channels, kernel_size)\n",
        "    kernel = kernel.permute(1, 0, 2)\n",
        "\n",
        "    # Apply convolution\n",
        "    h_all = F.conv1d(s_padded, kernel, groups=H)\n",
        "\n",
        "    # Permute back to (N, T, H)\n",
        "    h_all = h_all.permute(0, 2, 1)\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    return h_all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmcD4eJPytxG"
      },
      "source": [
        "# Sanity Check\n",
        "We can compare the outputs of the two implementations to check if they are consistent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "execution": {
          "iopub.execute_input": "2025-03-12T21:21:23.176996Z",
          "iopub.status.busy": "2025-03-12T21:21:23.176662Z",
          "iopub.status.idle": "2025-03-12T21:21:24.480211Z",
          "shell.execute_reply": "2025-03-12T21:21:24.479403Z",
          "shell.execute_reply.started": "2025-03-12T21:21:23.176970Z"
        },
        "id": "cbBabsamytxG",
        "outputId": "988d7ec0-30f8-4f99-d920-917581cd5949",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (4x4 and 2x4)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1814800328.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nMax absolute difference:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0msanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1814800328.py\u001b[0m in \u001b[0;36msanity_check\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mh_unrolled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munrolled_ssm_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mh_conv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_ssm_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1737897352.py\u001b[0m in \u001b[0;36munrolled_ssm_forward\u001b[0;34m(W, U, b, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mh_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (4x4 and 2x4)"
          ]
        }
      ],
      "source": [
        "def sanity_check():\n",
        "    T = 8   # number of time steps\n",
        "    H = 4   # hidden dimension\n",
        "    D = 3   # input dimension\n",
        "    N = 2\n",
        "\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    W = torch.randn(H, H) * 0.1\n",
        "    U = torch.randn(H, D) * 0.1\n",
        "    b = torch.randn(H) * 0.1\n",
        "\n",
        "    x = torch.randn(N, T, D)\n",
        "\n",
        "    h_unrolled = unrolled_ssm_forward(W, U, b, x)\n",
        "    h_conv = conv_ssm_forward(W, U, b, x)\n",
        "\n",
        "    diff = (h_unrolled - h_conv).abs().max()\n",
        "    print(\"Unrolled h(t):\")\n",
        "    print(h_unrolled)\n",
        "    print(\"\\nConv-based h(t):\")\n",
        "    print(h_conv)\n",
        "    print(\"\\nMax absolute difference:\", diff.item())\n",
        "\n",
        "sanity_check()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_bxNqV5ytxG"
      },
      "source": [
        "### Question 1\n",
        "\n",
        "What maximum absolute difference do you observe between the outputs of the two implementations for the following inputs?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axpXeGcNytxG"
      },
      "source": [
        "# Implementation Complexity\n",
        "\n",
        "We will now compare the two implementation in terms of their runtime efficiencies. But before we proceed, answer the following question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkj5iKBuytxG"
      },
      "source": [
        "### Question 2\n",
        "\n",
        "What is the number of operations being performed in the recurrence based implementation of the SSM forward pass?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftJVGyjNytxG"
      },
      "source": [
        "### Question 3\n",
        "\n",
        "What is the number of operations being performed in the convolution based implementation of the SSM forward pass?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqWy6JE377qq"
      },
      "source": [
        "### Question 4\n",
        "\n",
        "Compare the trade-off if any between the two implementations based on your answers above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmvZN9SR4sph"
      },
      "source": [
        "# Runtime Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-12T21:26:02.117139Z",
          "iopub.status.busy": "2025-03-12T21:26:02.116802Z",
          "iopub.status.idle": "2025-03-12T21:26:47.377750Z",
          "shell.execute_reply": "2025-03-12T21:26:47.377035Z",
          "shell.execute_reply.started": "2025-03-12T21:26:02.117113Z"
        },
        "id": "Vgh6sC3p5UqJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact\n",
        "\n",
        "def measure_runtime(method_fn, W, U, b, x, warmup=1, repeats=10):\n",
        "    # Warm-up runs (ignored in timing):\n",
        "    for _ in range(warmup):\n",
        "        method_fn(W, U, b, x)\n",
        "\n",
        "    # Timed runs:\n",
        "    start = time.time()\n",
        "    for _ in range(repeats):\n",
        "        method_fn(W, U, b, x)\n",
        "    end = time.time()\n",
        "\n",
        "    avg_time = (end - start) / repeats\n",
        "    return avg_time\n",
        "\n",
        "\n",
        "def run():\n",
        "    T_values_cache = {}\n",
        "    times_unrolled_vs_T_cache = {}\n",
        "    times_conv_vs_T_cache = {}\n",
        "\n",
        "    for H in [2, 4, 8, 16, 32]:\n",
        "      # We'll keep D, N fixed\n",
        "      D = 32\n",
        "      N = 32\n",
        "\n",
        "      T_values = [8, 32, 128, 256, 512]\n",
        "\n",
        "      # Build random U, b\n",
        "      U = torch.randn(H, D)*0.1\n",
        "      b = torch.randn(H)*0.1\n",
        "\n",
        "      times_unrolled_vs_T = []\n",
        "      times_conv_vs_T = []\n",
        "\n",
        "      for T in T_values:\n",
        "\n",
        "          diag_vals = torch.randn(H)*0.05\n",
        "          W = torch.randn(H, H)*0.05\n",
        "          x = torch.randn(N, T, D)\n",
        "\n",
        "          t_unrolled = measure_runtime(unrolled_ssm_forward, W, U, b, x)\n",
        "\n",
        "          t_conv = measure_runtime(conv_ssm_forward, W, U, b, x)\n",
        "\n",
        "          times_unrolled_vs_T.append(t_unrolled)\n",
        "          times_conv_vs_T.append(t_conv)\n",
        "\n",
        "      T_values_cache[H] = T_values\n",
        "      times_unrolled_vs_T_cache[H] = times_unrolled_vs_T\n",
        "      times_conv_vs_T_cache[H] = times_conv_vs_T\n",
        "    return T_values_cache, times_unrolled_vs_T_cache, times_conv_vs_T_cache\n",
        "\n",
        "T_values_cache, times_unrolled_vs_T_cache, times_conv_vs_T_cache = run()\n",
        "\n",
        "@interact(H=widgets.FloatLogSlider(min=1, max=5, base=2, value=4, step=1))\n",
        "def interactive_benchmark(H):\n",
        "    \"\"\"\n",
        "    Compare unrolled vs. diagonal-convolution RNN forward for various T,\n",
        "    at a chosen hidden dimension H from the slider.\n",
        "    \"\"\"\n",
        "    H = int(H)\n",
        "    T_values = T_values_cache[H]\n",
        "    T_unrolled = times_unrolled_vs_T_cache[H]\n",
        "    T_conv = times_conv_vs_T_cache[H]\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(T_values, T_unrolled, label=\"Unrolled\", marker='o')\n",
        "    plt.plot(T_values, T_conv, label=\"Conv\", marker='s')\n",
        "    plt.title(f\"Runtime vs T, H={H}\")\n",
        "    plt.xlabel(\"Time Steps (T)\")\n",
        "    plt.ylabel(\"Runtime (sec)\")\n",
        "    plt.yscale('log')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myO0dI0p4sph"
      },
      "source": [
        "### Question 5\n",
        "\n",
        "What do you observe about the runtime of the two implementations as $T$ and $H$ increase? Explain your finding."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "npy",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}